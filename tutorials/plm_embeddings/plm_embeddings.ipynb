{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Protein Language Model Embeddings with ESM-2\n",
                "\n",
                "**Run this notebook on Google Colab:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elkins/synth-pdb/blob/master/docs/tutorials/plm_embeddings.ipynb)\n",
                "\n",
                "---\n",
                "\n",
                "> **Who is this for?** Chemists and biologists who are curious about machine learning\n",
                "> applied to protein sequences -- no prior ML experience required.\n",
                "> Every technical concept is explained in plain language as it first appears.\n",
                "\n",
                "## Background: What is a Protein Language Model?\n",
                "\n",
                "A **language model** is software that learns patterns from vast amounts of text.\n",
                "ChatGPT, for example, was trained on billions of sentences and learned grammar,\n",
                "facts, and reasoning from the statistical patterns in that text.\n",
                "\n",
                "A **protein language model (PLM)** does exactly the same thing -- but its\n",
                "\"alphabet\" is the 20 standard amino acids (A, C, D, ... Y) instead of English words.\n",
                "ESM-2 (Evolutionary Scale Modelling, version 2) was trained on **250 million\n",
                "protein sequences** from the UniRef database.\n",
                "\n",
                "During training, the model played a fill-in-the-blank game: random residues were\n",
                "masked out, and the model had to predict them from context. To succeed, it had to\n",
                "implicitly learn *evolutionary constraints*, *structural preferences*, and\n",
                "*functional signals* for every position in a sequence.\n",
                "\n",
                "**Key insight**: ESM-2 never sees 3D coordinates during training -- yet its\n",
                "internal representations encode structural information anyway, because\n",
                "**sequence and structure are deeply coupled through evolution**.\n",
                "\n",
                "---\n",
                "\n",
                "## What you will learn\n",
                "\n",
                "This notebook demonstrates **ESM-2 protein language model embeddings** via `synth-pdb`.\n",
                "\n",
                "| What is encoded | Why it is useful |\n",
                "|---|---|\n",
                "| **Evolutionary conservation** | Co-varying positions across species |\n",
                "| **Structural context** | Buried vs. solvent-exposed |\n",
                "| **Chemical environment** | Charged / polar / hydrophobic neighbourhoods |\n",
                "| **Functional signals** | Active-site residues |\n",
                "\n",
                "All from **sequence alone** -- no 3D coordinates.\n",
                "\n",
                "### Notebook outline\n",
                "1. Setup & Installation\n",
                "2. Generate synthetic protein structures with `synth-pdb`\n",
                "3. Embed sequences with ESM-2 -- `(L, 320)` float32 arrays\n",
                "4. **Visualise** per-residue embedding heatmaps\n",
                "5. **Similarity heatmap** -- compare proteins in embedding space\n",
                "6. **Cluster** residues in 2D with UMAP\n",
                "7. **Linear structural probe** -- predict secondary structure from embeddings alone\n",
                "8. Embed a structure generated by synth-pdb\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation\n",
                "\n",
                "> **âš ï¸ Colab users:** The setup cell installs dependencies and **automatically restarts the kernel** once. This is expected â€” just wait ~10 seconds and **Run All Cells again**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title Setup & Installation { display-mode: \"form\" }\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# â”€â”€ Local development path (ignored on Colab) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "try:\n",
                "    repo_root = Path(\".\").resolve().parent.parent\n",
                "    if (repo_root / \"synth_pdb\").exists():\n",
                "        if str(repo_root) not in sys.path:\n",
                "            sys.path.insert(0, str(repo_root))\n",
                "            print(f\"ðŸ“Œ Local library: {repo_root}\")\n",
                "except Exception:\n",
                "    pass\n",
                "\n",
                "# â”€â”€ Colab installation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "if 'google.colab' in str(get_ipython()):\n",
                "    if not os.path.exists(\"plm_installed.marker\"):\n",
                "        print(\"ðŸ”§ Installing synth-pdb[plm] and visualisation dependencies ...\")\n",
                "        get_ipython().run_line_magic('pip', 'install -q \"synth-pdb[plm]\" umap-learn matplotlib seaborn')\n",
                "        with open(\"plm_installed.marker\", \"w\") as f:\n",
                "            f.write(\"done\")\n",
                "        print(\"ðŸ”„ Installation complete. KERNEL RESTARTING ...\")\n",
                "        print(\"âš ï¸  Wait ~10 seconds, then Run All Cells again.\")\n",
                "        os.kill(os.getpid(), 9)\n",
                "    else:\n",
                "        print(\"âœ… Dependencies ready.\")\n",
                "else:\n",
                "    import synth_pdb\n",
                "    print(f\"âœ… Local synth-pdb {synth_pdb.__version__}\")\n",
                "    print(\"   Make sure you have installed: pip install 'synth-pdb[plm]' umap-learn seaborn\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What is an 'embedding'?\n",
                "\n",
                "An **embedding** is a list of numbers (a *vector*) that represents something\n",
                "in a form a computer can do maths on. Think of it as a coordinate in a\n",
                "very high-dimensional space, where *similar things end up close together*.\n",
                "\n",
                "For example, in word embeddings, the vectors for *king* and *queen* point in\n",
                "similar directions, and so do *cat* and *dog*. In protein embeddings, residues\n",
                "that play similar biochemical roles -- e.g., positively charged Lys and Arg --\n",
                "end up in nearby regions of the embedding space.\n",
                "\n",
                "ESM-2 produces one 320-number vector per residue. A 20-residue peptide therefore\n",
                "gives a matrix of shape **20 x 320**. The 320 dimensions have no single human-\n",
                "interpretable meaning -- they are learned automatically by the model to be useful\n",
                "for predicting masked residues.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.colors as mcolors\n",
                "import seaborn as sns\n",
                "from synth_pdb.plm import ESM2Embedder\n",
                "\n",
                "# Instantiate the embedder â€” model loads lazily on first embed() call\n",
                "embedder = ESM2Embedder()\n",
                "print(f\"ESM2Embedder ready (model: {embedder.model_name})\")\n",
                "print(f\"Embedding dim: {embedder.embedding_dim}  (will be 320 for the default t6_8M model)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Synthetic Structures with synth-pdb\n",
                "\n",
                "We use `synth-pdb` to create three **homopolymers** -- peptides made of a single\n",
                "amino acid type repeated many times. Why homopolymers?\n",
                "\n",
                "- **Controlled test cases**: every residue sees the *same* local sequence context,\n",
                "  so differences in embeddings between the three homopolymers must arise from\n",
                "  amino acid identity, not from varying neighbours.\n",
                "- **Known secondary structure preferences**: Ala strongly prefers alpha-helices;\n",
                "  Val (beta-branched, bulky side chain) strongly prefers beta-strands;\n",
                "  Gly (no side chain) is the most flexible and tends to form coil or turns.\n",
                "\n",
                "| Protein | Sequence | Expected conformation | Chemical reason |\n",
                "|---|---|---|---|\n",
                "| Polyalanine | `AAAAAAAAAAAAAAAAAA` (18 res) | alpha-helix | Ala has the highest helix propensity of all amino acids |\n",
                "| Polyvaline | `VVVVVVVVVVVVVVVVVV` (18 res) | beta-strand | Val's beta-branching clashes sterically in a helix |\n",
                "| Polyglycine | `GGGGGGGGGGGGGGGGGG` (18 res) | Coil | Gly has no side chain, giving it near-unrestricted backbone freedom |\n",
                "| Ubiquitin N-term | `MQIFVKTLTGKTITLEVEPSDT` (22 res) | Mixed | Fragment of a real 76-residue eukaryotic regulatory protein |\n",
                "\n",
                "> **Helix propensity** (Pace & Scholtz, 1998, *Biophysical Journal*) is the\n",
                "> thermodynamic tendency of an amino acid to adopt an alpha-helical backbone\n",
                "> conformation (phi ~-57 degrees, psi ~-47 degrees). It can be measured\n",
                "> experimentally by substituting single residues into an alanine-based host peptide\n",
                "> and measuring the change in helix content by circular dichroism (CD) spectroscopy.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Our test sequences â€” we'll embed these and compare\n",
                "sequences = {\n",
                "    \"Poly-Ala (helix)\": \"AAAAAAAAAAAAAAAAAA\",\n",
                "    \"Poly-Val (strand)\": \"VVVVVVVVVVVVVVVVVV\",\n",
                "    \"Poly-Gly (coil)\": \"GGGGGGGGGGGGGGGGGG\",\n",
                "    \"Ubiquitin N-term\": \"MQIFVKTLTGKTITLEVEPSDT\",\n",
                "    \"Villin HP35\": \"LSDEDFKAVFGMTRSAFANLPLWKQQNLKKEKGLF\",\n",
                "    \"Trp-cage\": \"NLYIQWLKDGGPSSGRPPPS\",\n",
                "}\n",
                "\n",
                "print(\"Sequences loaded:\")\n",
                "for name, seq in sequences.items():\n",
                "    print(f\"  {name:25s}  {len(seq):3d} residues  {seq[:20]}{'...' if len(seq)>20 else ''}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embed with ESM-2\n",
                "\n",
                "The first call downloads the model weights (~30 MB for the smallest variant)\n",
                "and loads the transformer. Subsequent calls are fast (~10 ms per protein on CPU).\n",
                "\n",
                "**Output shape**: `(L, 320)` -- one 320-dimensional float32 vector per residue.\n",
                "\n",
                "### Under the hood -- how ESM-2 produces embeddings\n",
                "\n",
                "ESM-2 is a **Transformer** neural network (the same architecture behind ChatGPT,\n",
                "but Fine tuned for proteins). The key operation is *self-attention*:\n",
                "\n",
                "1. Each residue starts as a lookup vector based on its amino acid type (like a\n",
                "   dictionary: A maps to one vector, V maps to another, ...).\n",
                "2. The transformer stacks 6 layers. In each layer, every residue *attends* to\n",
                "   every other residue -- it updates its own representation by asking:\n",
                "   'which other positions in this sequence are most relevant to understanding me?'\n",
                "3. After 6 such rounds, each residue's 320-number vector encodes not just its own\n",
                "   identity but also what its sequence neighbourhood looks like.\n",
                "\n",
                "This is why an Ala in the middle of a helix gets a *different* vector than the same\n",
                "Ala next to a Pro (which breaks helices) -- context changes the embedding.\n",
                "\n",
                "**Reference**: Lin et al. (2023) *Evolutionary-scale prediction of atomic-level\n",
                "protein structure with a language model.* Science 379, 1123-1130.\n",
                "https://doi.org/10.1126/science.ade2574\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "print(\"Embedding sequences with ESM-2 t6_8M (320-dim) ...\")\n",
                "print(\"(First call downloads ~30 MB model weights â€” takes ~5-30 s)\\n\")\n",
                "\n",
                "embeddings = {}\n",
                "for name, seq in sequences.items():\n",
                "    t0 = time.time()\n",
                "    emb = embedder.embed(seq)                  # (L, 320) float32\n",
                "    dt_ms = (time.time() - t0) * 1000\n",
                "    embeddings[name] = emb\n",
                "    print(f\"  {name:25s}  shape={emb.shape}  dtype={emb.dtype}  {dt_ms:.1f} ms\")\n",
                "\n",
                "print(\"\\nâœ… All embeddings computed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Per-Residue Embedding Heatmaps\n",
                "\n",
                "Each row = one residue; each column = one of the 320 latent dimensions.\n",
                "The colour shows the activation value, normalised per row so the brightest\n",
                "colour in each row is 1.0 and the darkest is 0.0.\n",
                "\n",
                "**What to look for:**\n",
                "\n",
                "| Pattern | Interpretation |\n",
                "|---|---|\n",
                "| Uniform rows (same colour across a row) | Every residue sees the same context -- expected for a homopolymer |\n",
                "| Vertical stripes (same column highlighted for many rows) | That dimension is activated by a particular residue type |\n",
                "| Highly variable rows | Each residue has a unique chemical neighbourhood |\n",
                "\n",
                "Poly-Ala should look very uniform (all Ala, all identical context), while\n",
                "Ubiquitin N-term should have noticeably more variation between rows.\n",
                "\n",
                "> **Analogy for NMR spectroscopists**: these heatmaps are analogous to a 2D\n",
                "> HSQC spectrum, where each cross-peak encodes the chemical environment of one\n",
                "> NH group. Just as HSQC peak positions shift when you mutate a neighbouring\n",
                "> residue, the embedding changes when sequence context changes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for ax, (name, emb) in zip(axes, embeddings.items()):\n",
                "    # Normalise each row (residue) to [0,1] for visual comparison\n",
                "    row_min = emb.min(axis=1, keepdims=True)\n",
                "    row_max = emb.max(axis=1, keepdims=True)\n",
                "    normed = (emb - row_min) / (row_max - row_min + 1e-8)\n",
                "\n",
                "    im = ax.imshow(normed, aspect=\"auto\", cmap=\"RdBu_r\",\n",
                "                   interpolation=\"nearest\", vmin=0, vmax=1)\n",
                "    ax.set_title(f\"{name}\\n({emb.shape[0]} residues Ã— {emb.shape[1]} dims)\",\n",
                "                 fontsize=10, fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"ESM-2 embedding dimension\", fontsize=8)\n",
                "    ax.set_ylabel(\"Residue position\", fontsize=8)\n",
                "    ax.tick_params(labelsize=7)\n",
                "\n",
                "fig.colorbar(im, ax=axes[-1], label=\"Activation (row-normalised)\", shrink=0.8)\n",
                "fig.suptitle(\"ESM-2 Per-Residue Embedding Heatmaps\",\n",
                "             fontsize=14, fontweight=\"bold\", y=1.01)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\"\"\n",
                "Observations:\n",
                "  â€¢ Poly-Ala rows are nearly identical â€” every residue sees the same context.\n",
                "  â€¢ Ubiquitin rows vary noticeably â€” each residue has a unique neighborhood.\n",
                "  â€¢ Vertical stripes = dimensions strongly activated by a residue-type pattern.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sequence Similarity in Embedding Space\n",
                "\n",
                "We collapse each `(L, 320)` matrix to a single `(320,)` vector by **mean pooling**\n",
                "(averaging the 320-number vectors over all L residues). This gives one vector per\n",
                "protein representing the whole sequence.\n",
                "\n",
                "We then compute **cosine similarity** between all pairs:\n",
                "\n",
                "$$\\text{sim}(A, B) = \\frac{\\overline{\\text{emb}}(A) \\cdot \\overline{\\text{emb}}(B)}{\\|\\overline{\\text{emb}}(A)\\| \\cdot \\|\\overline{\\text{emb}}(B)\\|}$$\n",
                "\n",
                "### What is cosine similarity?\n",
                "\n",
                "Think of two arrows starting from the same point in 320-dimensional space.\n",
                "Cosine similarity measures the **angle** between them -- not their lengths:\n",
                "\n",
                "- **1.0** -- identical directions -- the two proteins look the same to ESM-2\n",
                "- **0.0** -- perpendicular -- completely uncorrelated representations\n",
                "- Proteins with the same amino acid composition but different order can still\n",
                "  have high similarity if they fold similarly.\n",
                "\n",
                "**What to expect:**\n",
                "- Poly-Ala and Poly-Val: **high** similarity -- both are simple, repetitive homopolymers\n",
                "- Poly-Gly: **lower** -- Gly is biochemically unique (only achiral amino acid,\n",
                "  no side chain, exceptional backbone flexibility)\n",
                "- Ubiquitin / Villin / Trp-cage: moderate -- all are real compact folded proteins\n",
                "\n",
                "> **Why not just use BLAST sequence identity?** Two proteins with <20% sequence\n",
                "> identity (the 'twilight zone') can still perform identical functions and have\n",
                "> the same fold. Cosine similarity in PLM embedding space can detect functional\n",
                "> similarity even between evolutionarily distant homologues where BLAST gives\n",
                "> no meaningful alignment.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "names = list(sequences.keys())\n",
                "n = len(names)\n",
                "\n",
                "# Compute mean embeddings  â†’  (N, 320)\n",
                "mean_embs = np.stack([embeddings[nm].mean(axis=0) for nm in names])\n",
                "\n",
                "# Normalise rows â†’ unit vectors, then dot product = cosine similarity\n",
                "norms = np.linalg.norm(mean_embs, axis=1, keepdims=True)\n",
                "normed = mean_embs / (norms + 1e-8)\n",
                "sim_matrix = normed @ normed.T   # (N, N)\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.heatmap(\n",
                "    sim_matrix,\n",
                "    xticklabels=names, yticklabels=names,\n",
                "    annot=True, fmt=\".3f\",\n",
                "    cmap=\"YlOrRd\", vmin=0.5, vmax=1.0,\n",
                "    linewidths=0.5,\n",
                "    ax=ax,\n",
                ")\n",
                "ax.set_title(\"ESM-2 Sequence Similarity (cosine, mean-pooled)\",\n",
                "             fontsize=13, fontweight=\"bold\", pad=12)\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\", fontsize=9)\n",
                "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print commentary\n",
                "for i, a in enumerate(names):\n",
                "    for j, b in enumerate(names):\n",
                "        if j > i:\n",
                "            s = sim_matrix[i, j]\n",
                "            label = \"high\" if s > 0.85 else (\"moderate\" if s > 0.75 else \"low\")\n",
                "            print(f\"  {a:25s}  â†”  {b:25s}  sim={s:.3f}  ({label})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Residue-Level UMAP Clustering\n",
                "\n",
                "Each residue is a point in 320-dimensional space. We use **UMAP** to project\n",
                "down to 2D so we can visualise it.\n",
                "\n",
                "### What is dimensionality reduction?\n",
                "\n",
                "Imagine flattening a globe onto a flat map -- you inevitably lose some\n",
                "information, but relative positions are preserved as faithfully as possible.\n",
                "UMAP (Uniform Manifold Approximation and Projection, McInnes et al. 2018) is a\n",
                "widely used algorithm that preserves *local neighbourhood structure*: if two\n",
                "residues are close in 320-D, they will be close in the 2D plot.\n",
                "\n",
                "**Key question**: do residues cluster by amino acid identity in this 2D map,\n",
                "even though ESM-2 was never explicitly told what a residue *is* -- only which\n",
                "positions were masked?\n",
                "\n",
                "The answer is yes. ESM-2 implicitly learns amino acid identity through the\n",
                "pattern of masked predictions. In UMAP space, Ala, Val, Gly etc. tend to form\n",
                "distinct clouds.\n",
                "\n",
                "> **Note on homopolymers**: Poly-Ala, Poly-Val, and Poly-Gly will each appear as a\n",
                "> **single tight dot** (all residues share identical context), while Ubiquitin and\n",
                "> Villin will appear as *clouds* of distinct points -- one point per residue.\n",
                "\n",
                "> **Reference**: McInnes, L., Healy, J., & Melville, J. (2018). *UMAP: Uniform\n",
                "> Manifold Approximation and Projection for Dimension Reduction.* arXiv:1802.03426.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import umap\n",
                "\n",
                "    # Stack all residue embeddings with labels\n",
                "    all_embs = []\n",
                "    all_labels = []\n",
                "    all_source = []\n",
                "\n",
                "    for name, seq in sequences.items():\n",
                "        emb = embeddings[name]       # (L, 320)\n",
                "        for i, (vec, aa) in enumerate(zip(emb, seq)):\n",
                "            all_embs.append(vec)\n",
                "            all_labels.append(aa)   # single-letter amino acid code\n",
                "            all_source.append(name)\n",
                "\n",
                "    X = np.stack(all_embs)           # (N_residues, 320)\n",
                "    labels = np.array(all_labels)    # amino acid 1-letter codes\n",
                "\n",
                "    print(f\"Running UMAP on {len(X)} residue embeddings (320-dim â†’ 2D) ...\")\n",
                "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=8, min_dist=0.3)\n",
                "    Z = reducer.fit_transform(X)     # (N_residues, 2)\n",
                "\n",
                "    # Colour by amino acid identity\n",
                "    unique_aas = sorted(set(labels))\n",
                "    cmap = plt.cm.get_cmap(\"tab20\", len(unique_aas))\n",
                "    color_map = {aa: cmap(i) for i, aa in enumerate(unique_aas)}\n",
                "    colors = [color_map[aa] for aa in labels]\n",
                "\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "    # Left: color by amino acid\n",
                "    ax = axes[0]\n",
                "    for aa in unique_aas:\n",
                "        mask = labels == aa\n",
                "        ax.scatter(Z[mask, 0], Z[mask, 1], c=[color_map[aa]],\n",
                "                   label=aa, s=40, alpha=0.75, edgecolors=\"white\", linewidths=0.3)\n",
                "    ax.legend(title=\"Amino acid\", bbox_to_anchor=(1.0, 1), loc=\"upper left\",\n",
                "              fontsize=8, title_fontsize=9, ncol=2)\n",
                "    ax.set_title(\"UMAP of ESM-2 residue embeddings\\nColoured by amino acid identity\",\n",
                "                 fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"UMAP-1\")\n",
                "    ax.set_ylabel(\"UMAP-2\")\n",
                "\n",
                "    # Right: color by source protein\n",
                "    ax = axes[1]\n",
                "    source_labels = np.array(all_source)\n",
                "    unique_sources = list(sequences.keys())\n",
                "    scmap = plt.cm.get_cmap(\"Set2\", len(unique_sources))\n",
                "    for i, src in enumerate(unique_sources):\n",
                "        mask = source_labels == src\n",
                "        ax.scatter(Z[mask, 0], Z[mask, 1], c=[scmap(i)],\n",
                "                   label=src, s=40, alpha=0.75, edgecolors=\"white\", linewidths=0.3)\n",
                "    ax.legend(title=\"Protein\", bbox_to_anchor=(1.0, 1), loc=\"upper left\",\n",
                "              fontsize=8, title_fontsize=9)\n",
                "    ax.set_title(\"UMAP of ESM-2 residue embeddings\\nColoured by source protein\",\n",
                "                 fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"UMAP-1\")\n",
                "    ax.set_ylabel(\"UMAP-2\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    print(\"\"\"\n",
                "What to look for:\n",
                "  â€¢ Same amino acid in different proteins â†’ clusters together   (ESM-2 learned residue identity)\n",
                "  â€¢ Points from the same protein â†’ may also cluster             (ESM-2 learned local context)\n",
                "  â€¢ Poly-Ala / Poly-Val / Poly-Gly â†’ tight single-residue dots (all identical context within each)\n",
                "\"\"\")\n",
                "\n",
                "except ImportError:\n",
                "    print(\"umap-learn not installed. Run: pip install umap-learn\")\n",
                "    print(\"Then re-run this cell.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Linear Secondary Structure Probe\n",
                "\n",
                "Can a **single linear layer** on top of ESM-2 embeddings predict secondary structure?\n",
                "\n",
                "### What is a 'linear probe'?\n",
                "\n",
                "A linear probe is the simplest possible ML classifier: it multiplies the embedding\n",
                "vector by a weight matrix and adds a bias -- just one matrix multiplication. If this\n",
                "step achieves high accuracy, secondary structure is *already explicitly encoded*\n",
                "in the embedding and requires no further non-linear processing to extract.\n",
                "\n",
                "**Analogy**: if you can read the temperature off a thermometer by looking at a\n",
                "number (linear), the information is visually explicit. If you needed to do a\n",
                "complex chemical assay (non-linear), the information would be deeply implicit.\n",
                "\n",
                "### The experiment\n",
                "\n",
                "We build a small labelled dataset from `synth-pdb`:\n",
                "- 30 residues of Poly-Ala (dominant helix tendency) -- label **0** (Helix)\n",
                "- 30 residues of Poly-Val (dominant strand tendency) -- label **1** (Strand)\n",
                "- 30 residues of Poly-Gly (dominant coil tendency) -- label **2** (Coil)\n",
                "\n",
                "Then a **logistic regression** (a linear layer followed by softmax to produce\n",
                "class probabilities) is evaluated by **5-fold cross-validation**:\n",
                "the data is split into 5 equal chunks; the model trains on 4 and tests\n",
                "on the 5th, rotating through all combinations. The 5 accuracy values are averaged\n",
                "to give a robust estimate that avoids memorising any single train/test split.\n",
                "\n",
                "> **Literature benchmark**: Rao et al. (2019) *Evaluating protein transfer learning\n",
                "> with TAPE* (NeurIPS) reported ~70-80% SS3 accuracy on real protein datasets with\n",
                "> a linear probe over ESM embeddings. Our synthetic homopolymer dataset is simpler,\n",
                "> so accuracy should be very high -- but the principle is identical.\n",
                "\n",
                "> **What to expect**: Near-perfect accuracy. The three homopolymers represent very\n",
                "> different biochemical environments; ESM-2 should encode helix/strand/coil\n",
                "> preferences linearly from the amino acid type alone.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import warnings\n",
                "\n",
                "# Build a labelled dataset of residue embeddings\n",
                "# Label 0 = helix (Ala)  |  Label 1 = strand (Val)  |  Label 2 = coil (Gly)\n",
                "probe_seqs = [\n",
                "    (\"A\" * 30, 0, \"Helix (Ala)\"),\n",
                "    (\"V\" * 30, 1, \"Strand (Val)\"),\n",
                "    (\"G\" * 30, 2, \"Coil (Gly)\"),\n",
                "    # Additional diversity: mixed sequences\n",
                "    (\"AAAVVVAAA\" * 3, None, \"Mixed\"),   # ground truth is ambiguous, skip\n",
                "]\n",
                "\n",
                "X_probe, y_probe = [], []\n",
                "for seq, label, desc in probe_seqs:\n",
                "    if label is None:\n",
                "        continue\n",
                "    emb = embedder.embed(seq)   # (L, 320)\n",
                "    for row in emb:\n",
                "        X_probe.append(row)\n",
                "        y_probe.append(label)\n",
                "\n",
                "X_probe = np.array(X_probe)\n",
                "y_probe = np.array(y_probe)\n",
                "print(f\"Dataset: {X_probe.shape[0]} residues Ã— {X_probe.shape[1]} embedding dims\")\n",
                "print(f\"Classes: 0=Helix ({np.sum(y_probe==0)}), 1=Strand ({np.sum(y_probe==1)}), 2=Coil ({np.sum(y_probe==2)})\")\n",
                "\n",
                "# Standardise features (logistic regression converges better)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_probe)\n",
                "\n",
                "# 5-fold cross-validation with a linear probe\n",
                "with warnings.catch_warnings():\n",
                "    warnings.simplefilter(\"ignore\")\n",
                "    clf = LogisticRegression(max_iter=500, C=1.0, random_state=42)\n",
                "    cv_scores = cross_val_score(clf, X_scaled, y_probe, cv=5, scoring=\"accuracy\")\n",
                "\n",
                "print(f\"\\n5-fold CV accuracy: {cv_scores.mean():.1%} Â± {cv_scores.std():.1%}\")\n",
                "print(f\"Individual folds: {[f'{s:.1%}' for s in cv_scores]}\")\n",
                "\n",
                "# Fit on full data and show confusion matrix\n",
                "clf.fit(X_scaled, y_probe)\n",
                "y_pred = clf.predict(X_scaled)\n",
                "\n",
                "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
                "cm = confusion_matrix(y_probe, y_pred)\n",
                "fig, ax = plt.subplots(figsize=(5, 4))\n",
                "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Helix (Ala)\", \"Strand (Val)\", \"Coil (Gly)\"])\n",
                "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
                "ax.set_title(f\"Linear Probe on ESM-2 Embeddings\\nCV accuracy: {cv_scores.mean():.1%}\",\n",
                "             fontweight=\"bold\", pad=10)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\"\"\n",
                "Interpretation:\n",
                "  High accuracy â†’ secondary structure is LINEARLY ENCODED in ESM-2 embeddings.\n",
                "  The model learned to distinguish helix/strand/coil from sequence context alone,\n",
                "  with no 3D coordinates in the training data.\n",
                "  This is the same principle that enables ESMFold to predict full 3D structure.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Embedding a Structure from synth-pdb\n",
                "\n",
                "We can also pass a **biotite `AtomArray`** (a table of atoms with 3D coordinates)\n",
                "directly to `embed_structure()`. The method extracts the amino acid sequence from\n",
                "the residue names and calls `embed()` under the hood.\n",
                "\n",
                "### Why combine structure generation + PLM embeddings?\n",
                "\n",
                "| synth-pdb gives you | ESM-2 gives you |\n",
                "|---|---|\n",
                "| Realistic 3D atomic coordinates | Sequence-derived feature vectors |\n",
                "| Backbone geometry (phi/psi angles, bond lengths) | Evolutionary & structural context per residue |\n",
                "| PDB format for downstream software | Compact 320-dim representation for ML |\n",
                "\n",
                "Combining both lets you build models that use *geometric features* (from 3D\n",
                "structure) **and** *sequence context features* (from ESM-2) -- a combination\n",
                "used in state-of-the-art protein quality assessment and design pipelines.\n",
                "\n",
                "> **Note**: `embed_structure()` only uses the **sequence** extracted from the\n",
                "> `AtomArray`; it ignores the 3D coordinates. To use both, extract the embeddings\n",
                "> here and compute geometric descriptors (angles, SASA etc.) separately, then\n",
                "> concatenate them as features.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from synth_pdb.generator import generate_pdb_content\n",
                "import biotite.structure.io.pdb as pdb\n",
                "import biotite.structure as struc\n",
                "import io\n",
                "\n",
                "# Generate a synthetic 20-residue peptide using synth-pdb.\n",
                "# 'conformation=\"alpha\"' sets all backbone torsion angles to canonical\n",
                "# alpha-helix values (phi ~ -57 deg, psi ~ -47 deg).\n",
                "# 'minimize_energy=False' skips OpenMM energy minimisation for speed.\n",
                "pdb_content = generate_pdb_content(\n",
                "    sequence_str=\"ALA-ALA-ALA-ALA-ALA-GLY-LEU-ALA-ALA-ALA-ALA-ALA-SER-ALA-ALA-ALA-ALA-ALA-ALA-ALA\",\n",
                "    conformation=\"alpha\",\n",
                "    minimize_energy=False,\n",
                ")\n",
                "\n",
                "# Parse the PDB text into a biotite AtomArray --\n",
                "# a structured table of atoms with coordinates, chain IDs, residue names, etc.\n",
                "pdb_file = pdb.PDBFile.read(io.StringIO(pdb_content))\n",
                "structure = pdb_file.get_structure(model=1)\n",
                "\n",
                "# Keep only backbone atoms (N, C-alpha, C, O) to discard hydrogens\n",
                "# and side chain atoms. For embedding we only need the residue sequence,\n",
                "# but filtering to backbone atoms is good practice for downstream geometry work.\n",
                "backbone = structure[np.isin(structure.atom_name, [\"N\", \"CA\", \"C\", \"O\"])]\n",
                "\n",
                "print(f\"Generated structure: {len(struc.get_residues(backbone)[0])} residues, {len(backbone)} atoms\")\n",
                "\n",
                "# embed_structure() reads the 3-letter residue names from the AtomArray,\n",
                "# converts them to 1-letter codes (ALA->A, GLY->G, etc.), then calls embed().\n",
                "# Output: (L, 320) float32 numpy array -- one embedding vector per residue.\n",
                "struct_emb = embedder.embed_structure(backbone)\n",
                "print(f\"PLM embedding shape: {struct_emb.shape}\")\n",
                "print(f\"dtype: {struct_emb.dtype}\")\n",
                "\n",
                "# Plot the embedding matrix transposed so residues run along the x-axis.\n",
                "# Each vertical slice is the 320-number 'fingerprint' for one residue.\n",
                "fig, ax = plt.subplots(figsize=(12, 3))\n",
                "im = ax.imshow(struct_emb.T, aspect=\"auto\", cmap=\"RdBu_r\",\n",
                "               interpolation=\"nearest\")\n",
                "ax.set_xlabel(\"Residue position\", fontsize=10)\n",
                "ax.set_ylabel(\"ESM-2 dimension (320)\", fontsize=10)\n",
                "ax.set_title(\"synth-pdb -> ESM-2 embedding:  20-residue synthetic helix\",\n",
                "             fontsize=11, fontweight=\"bold\")\n",
                "plt.colorbar(im, ax=ax, label=\"Activation\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nThe embedder extracted the sequence from the AtomArray residue names,\")\n",
                "print(\"passed it through ESM-2, and returned a (L, 320) float32 matrix.\")\n",
                "print(\"This can be used directly as node features in a GNN, or concatenated\")\n",
                "print(\"with geometric features (bond angles, SASA, etc.) for richer input representations.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary & Next Steps\n",
                "\n",
                "### What we demonstrated\n",
                "\n",
                "| Step | Result |\n",
                "|---|---|\n",
                "| `embedder.embed(seq)` | `(L, 320)` per-residue float32 matrix |\n",
                "| `embedder.embed_structure(arr)` | Same, directly from `AtomArray` |\n",
                "| Heatmaps | Poly-Ala uniform; ubiquitin diverse â€” context is encoded |\n",
                "| Similarity matrix | Repetitive peptides cluster; real proteins moderate similarity |\n",
                "| UMAP | Residues cluster by amino acid identity in 2D |\n",
                "| Linear probe | High SS accuracy â†’ structural info **linearly accessible** |\n",
                "\n",
                "### Using a more powerful model\n",
                "\n",
                "The API is identical for all ESM-2 variants:\n",
                "\n",
                "```python\n",
                "# Current: 8M params, 320-dim\n",
                "embedder = ESM2Embedder()   \n",
                "\n",
                "# Better: 35M params, 480-dim (~140 MB)\n",
                "embedder = ESM2Embedder(model_name=\"facebook/esm2_t12_35M_UR50D\")\n",
                "\n",
                "# Best for research: 650M params, 1280-dim (~2.5 GB, use GPU)\n",
                "embedder = ESM2Embedder(model_name=\"facebook/esm2_t33_650M_UR50D\", device=\"cuda\")\n",
                "```\n",
                "\n",
                "### Downstream integration ideas\n",
                "\n",
                "```python\n",
                "# 1. Feed into the synth-pdb GNN quality scorer as node features\n",
                "plm_feats = embedder.embed_structure(structure)      # (L, 320)\n",
                "node_features = np.concatenate([geo_feats, plm_feats], axis=-1)\n",
                "\n",
                "# 2. Pairwise contact prediction (outer product trick)\n",
                "emb = embedder.embed(sequence)                       # (L, 320)\n",
                "outer = np.einsum('id,jd->ij', emb, emb)            # (L, L) â€” feed into CNN\n",
                "\n",
                "# 3. Retrieval by function similarity\n",
                "sim = embedder.sequence_similarity(query_seq, candidate_seq)\n",
                "```\n",
                "\n",
                "### Reference\n",
                "\n",
                "Lin, Z. et al. (2023) *Evolutionary-scale prediction of atomic-level protein structure with a language model.* Science 379, 1123â€“1130.  \n",
                "https://doi.org/10.1126/science.ade2574"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true,
            "name": "plm_embeddings.ipynb"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}