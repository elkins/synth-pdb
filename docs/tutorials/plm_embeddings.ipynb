{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Protein Language Model Embeddings with ESM-2 ðŸ§¬\n",
                "\n",
                "**Run this notebook on Google Colab:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elkins/synth-pdb/blob/main/docs/tutorials/plm_embeddings.ipynb)\n",
                "\n",
                "---\n",
                "\n",
                "## What you'll learn\n",
                "\n",
                "This notebook demonstrates **ESM-2 protein language model embeddings** via `synth-pdb`.\n",
                "\n",
                "A protein language model (PLM) is a transformer trained on 250 million protein sequences.\n",
                "After training, its *internal activations* form per-residue vectors that encode:\n",
                "\n",
                "| What is encoded | Why it's useful |\n",
                "|---|---|\n",
                "| **Evolutionary conservation** | Co-varying positions across species |\n",
                "| **Structural context** | Buried vs. solvent-exposed |\n",
                "| **Chemical environment** | Charged / polar / hydrophobic neighbourhoods |\n",
                "| **Functional signals** | Active-site residues |\n",
                "\n",
                "All from **sequence alone** â€” no 3D coordinates.\n",
                "\n",
                "### Notebook outline\n",
                "1. Setup & Installation\n",
                "2. Generate synthetic protein structures with `synth-pdb`\n",
                "3. Embed sequences with ESM-2 â†’ `(L, 320)` float32 arrays\n",
                "4. **Visualise** per-residue embedding heatmaps\n",
                "5. **Cluster** proteins in embedding space with UMAP\n",
                "6. **Similarity heatmap** â€” polyalanine vs. polyvaline vs. ubiquitin\n",
                "7. **Linear structural probe** â€” predict secondary structure from embeddings alone\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation\n",
                "\n",
                "> **âš ï¸ Colab users:** The setup cell installs dependencies and **automatically restarts the kernel** once. This is expected â€” just wait ~10 seconds and **Run All Cells again**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title Setup & Installation { display-mode: \"form\" }\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# â”€â”€ Local development path (ignored on Colab) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "try:\n",
                "    repo_root = Path(\".\").resolve().parent.parent\n",
                "    if (repo_root / \"synth_pdb\").exists():\n",
                "        if str(repo_root) not in sys.path:\n",
                "            sys.path.insert(0, str(repo_root))\n",
                "            print(f\"ðŸ“Œ Local library: {repo_root}\")\n",
                "except Exception:\n",
                "    pass\n",
                "\n",
                "# â”€â”€ Colab installation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "if 'google.colab' in str(get_ipython()):\n",
                "    if not os.path.exists(\"plm_installed.marker\"):\n",
                "        print(\"ðŸ”§ Installing synth-pdb[plm] and visualisation dependencies ...\")\n",
                "        get_ipython().run_line_magic('pip', 'install -q \"synth-pdb[plm]\" umap-learn matplotlib seaborn')\n",
                "        with open(\"plm_installed.marker\", \"w\") as f:\n",
                "            f.write(\"done\")\n",
                "        print(\"ðŸ”„ Installation complete. KERNEL RESTARTING ...\")\n",
                "        print(\"âš ï¸  Wait ~10 seconds, then Run All Cells again.\")\n",
                "        os.kill(os.getpid(), 9)\n",
                "    else:\n",
                "        print(\"âœ… Dependencies ready.\")\n",
                "else:\n",
                "    import synth_pdb\n",
                "    print(f\"âœ… Local synth-pdb {synth_pdb.__version__}\")\n",
                "    print(\"   Make sure you have installed: pip install 'synth-pdb[plm]' umap-learn seaborn\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.colors as mcolors\n",
                "import seaborn as sns\n",
                "from synth_pdb.plm import ESM2Embedder\n",
                "\n",
                "# Instantiate the embedder â€” model loads lazily on first embed() call\n",
                "embedder = ESM2Embedder()\n",
                "print(f\"ESM2Embedder ready (model: {embedder.model_name})\")\n",
                "print(f\"Embedding dim: {embedder.embedding_dim}  (will be 320 for the default t6_8M model)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Synthetic Structures with synth-pdb\n",
                "\n",
                "We use `synth-pdb` to generate three canonical secondary structures so we can compare\n",
                "how their sequences are encoded differently in PLM space.\n",
                "\n",
                "| Protein | Sequence | Expected conformation |\n",
                "|---|---|---|\n",
                "| Polyalanine | `AAAAAAAAAAAAAAAAAA` (18 residues) | Î±-helix (Ala strongly prefers helix) |\n",
                "| Polyvaline | `VVVVVVVVVVVVVVVVVV` (18 residues) | Î²-strand (Val Î²-branching disfavours helix) |\n",
                "| Polyglycine | `GGGGGGGGGGGGGGGGGG` (18 residues) | Coil (Gly has no side chain â†’ most flexible) |\n",
                "| Ubiquitin N-term | `MQIFVKTLTGKTITLEVEPSDT` (22 residues) | Mixed: helix + strand |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Our test sequences â€” we'll embed these and compare\n",
                "sequences = {\n",
                "    \"Poly-Ala (helix)\": \"AAAAAAAAAAAAAAAAAA\",\n",
                "    \"Poly-Val (strand)\": \"VVVVVVVVVVVVVVVVVV\",\n",
                "    \"Poly-Gly (coil)\": \"GGGGGGGGGGGGGGGGGG\",\n",
                "    \"Ubiquitin N-term\": \"MQIFVKTLTGKTITLEVEPSDT\",\n",
                "    \"Villin HP35\": \"LSDEDFKAVFGMTRSAFANLPLWKQQNLKKEKGLF\",\n",
                "    \"Trp-cage\": \"NLYIQWLKDGGPSSGRPPPS\",\n",
                "}\n",
                "\n",
                "print(\"Sequences loaded:\")\n",
                "for name, seq in sequences.items():\n",
                "    print(f\"  {name:25s}  {len(seq):3d} residues  {seq[:20]}{'...' if len(seq)>20 else ''}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embed with ESM-2\n",
                "\n",
                "The first call downloads the model weights (~30 MB) and loads the transformer.\n",
                "Subsequent calls are fast (~10 ms per protein on CPU).\n",
                "\n",
                "**Output shape**: `(L, 320)` â€” one 320-dimensional float32 vector per residue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "print(\"Embedding sequences with ESM-2 t6_8M (320-dim) ...\")\n",
                "print(\"(First call downloads ~30 MB model weights â€” takes ~5-30 s)\\n\")\n",
                "\n",
                "embeddings = {}\n",
                "for name, seq in sequences.items():\n",
                "    t0 = time.time()\n",
                "    emb = embedder.embed(seq)                  # (L, 320) float32\n",
                "    dt_ms = (time.time() - t0) * 1000\n",
                "    embeddings[name] = emb\n",
                "    print(f\"  {name:25s}  shape={emb.shape}  dtype={emb.dtype}  {dt_ms:.1f} ms\")\n",
                "\n",
                "print(\"\\nâœ… All embeddings computed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Per-Residue Embedding Heatmaps\n",
                "\n",
                "Each row = one residue, each column = one of the 320 latent dimensions.\n",
                "\n",
                "**What to look for:**\n",
                "- A **uniform heatmap** for polyalanine â€” all residues see identical local contexts\n",
                "- **More variation** for ubiquitin â€” each residue has a unique chemical neighbourhood\n",
                "- Coloured bands = dimensions consistently activated by a particular residue type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for ax, (name, emb) in zip(axes, embeddings.items()):\n",
                "    # Normalise each row (residue) to [0,1] for visual comparison\n",
                "    row_min = emb.min(axis=1, keepdims=True)\n",
                "    row_max = emb.max(axis=1, keepdims=True)\n",
                "    normed = (emb - row_min) / (row_max - row_min + 1e-8)\n",
                "\n",
                "    im = ax.imshow(normed, aspect=\"auto\", cmap=\"RdBu_r\",\n",
                "                   interpolation=\"nearest\", vmin=0, vmax=1)\n",
                "    ax.set_title(f\"{name}\\n({emb.shape[0]} residues Ã— {emb.shape[1]} dims)\",\n",
                "                 fontsize=10, fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"ESM-2 embedding dimension\", fontsize=8)\n",
                "    ax.set_ylabel(\"Residue position\", fontsize=8)\n",
                "    ax.tick_params(labelsize=7)\n",
                "\n",
                "fig.colorbar(im, ax=axes[-1], label=\"Activation (row-normalised)\", shrink=0.8)\n",
                "fig.suptitle(\"ESM-2 Per-Residue Embedding Heatmaps\",\n",
                "             fontsize=14, fontweight=\"bold\", y=1.01)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\"\"\n",
                "Observations:\n",
                "  â€¢ Poly-Ala rows are nearly identical â€” every residue sees the same context.\n",
                "  â€¢ Ubiquitin rows vary noticeably â€” each residue has a unique neighbourhood.\n",
                "  â€¢ Vertical stripes = dimensions strongly activated by a residue-type pattern.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sequence Similarity in Embedding Space\n",
                "\n",
                "We collapse each `(L, 320)` matrix to a single `(320,)` vector by **mean pooling**,\n",
                "then compute **cosine similarity** between all pairs.\n",
                "\n",
                "$$\\text{sim}(A, B) = \\frac{\\overline{\\text{emb}}(A) \\cdot \\overline{\\text{emb}}(B)}{\\|\\overline{\\text{emb}}(A)\\| \\cdot \\|\\overline{\\text{emb}}(B)\\|}$$\n",
                "\n",
                "**What to expect:**\n",
                "- Poly-Ala and Poly-Val: **high** similarity (both simple repetitive peptides)\n",
                "- Poly-Gly: **lower** similarity to others (Gly is biochemically special â€” smallest residue)\n",
                "- Ubiquitin / Villin / Trp-cage: moderate similarity (all are real folded proteins)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "names = list(sequences.keys())\n",
                "n = len(names)\n",
                "\n",
                "# Compute mean embeddings  â†’  (N, 320)\n",
                "mean_embs = np.stack([embeddings[nm].mean(axis=0) for nm in names])\n",
                "\n",
                "# Normalise rows â†’ unit vectors, then dot product = cosine similarity\n",
                "norms = np.linalg.norm(mean_embs, axis=1, keepdims=True)\n",
                "normed = mean_embs / (norms + 1e-8)\n",
                "sim_matrix = normed @ normed.T   # (N, N)\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.heatmap(\n",
                "    sim_matrix,\n",
                "    xticklabels=names, yticklabels=names,\n",
                "    annot=True, fmt=\".3f\",\n",
                "    cmap=\"YlOrRd\", vmin=0.5, vmax=1.0,\n",
                "    linewidths=0.5,\n",
                "    ax=ax,\n",
                ")\n",
                "ax.set_title(\"ESM-2 Sequence Similarity (cosine, mean-pooled)\",\n",
                "             fontsize=13, fontweight=\"bold\", pad=12)\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\", fontsize=9)\n",
                "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print commentary\n",
                "for i, a in enumerate(names):\n",
                "    for j, b in enumerate(names):\n",
                "        if j > i:\n",
                "            s = sim_matrix[i, j]\n",
                "            label = \"high\" if s > 0.85 else (\"moderate\" if s > 0.75 else \"low\")\n",
                "            print(f\"  {a:25s}  â†”  {b:25s}  sim={s:.3f}  ({label})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Residue-Level UMAP Clustering\n",
                "\n",
                "Each residue is a point in 320-dimensional space. We use **UMAP** to project down to 2D.\n",
                "\n",
                "**Key question**: do residues cluster by amino acid identity, even though the model\n",
                "was never told what a residue *is* â€” only which positions are masked?\n",
                "\n",
                "The answer is yes: ESM-2 implicitly learns amino acid identity through the pattern\n",
                "of masked predictions. In UMAP space, Ala, Val, Gly etc. should form distinct clouds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import umap\n",
                "\n",
                "    # Stack all residue embeddings with labels\n",
                "    all_embs = []\n",
                "    all_labels = []\n",
                "    all_source = []\n",
                "\n",
                "    for name, seq in sequences.items():\n",
                "        emb = embeddings[name]       # (L, 320)\n",
                "        for i, (vec, aa) in enumerate(zip(emb, seq)):\n",
                "            all_embs.append(vec)\n",
                "            all_labels.append(aa)   # single-letter amino acid code\n",
                "            all_source.append(name)\n",
                "\n",
                "    X = np.stack(all_embs)           # (N_residues, 320)\n",
                "    labels = np.array(all_labels)    # amino acid 1-letter codes\n",
                "\n",
                "    print(f\"Running UMAP on {len(X)} residue embeddings (320-dim â†’ 2D) ...\")\n",
                "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=8, min_dist=0.3)\n",
                "    Z = reducer.fit_transform(X)     # (N_residues, 2)\n",
                "\n",
                "    # Colour by amino acid identity\n",
                "    unique_aas = sorted(set(labels))\n",
                "    cmap = plt.cm.get_cmap(\"tab20\", len(unique_aas))\n",
                "    color_map = {aa: cmap(i) for i, aa in enumerate(unique_aas)}\n",
                "    colors = [color_map[aa] for aa in labels]\n",
                "\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "    # Left: colour by amino acid\n",
                "    ax = axes[0]\n",
                "    for aa in unique_aas:\n",
                "        mask = labels == aa\n",
                "        ax.scatter(Z[mask, 0], Z[mask, 1], c=[color_map[aa]],\n",
                "                   label=aa, s=40, alpha=0.75, edgecolors=\"white\", linewidths=0.3)\n",
                "    ax.legend(title=\"Amino acid\", bbox_to_anchor=(1.0, 1), loc=\"upper left\",\n",
                "              fontsize=8, title_fontsize=9, ncol=2)\n",
                "    ax.set_title(\"UMAP of ESM-2 residue embeddings\\nColoured by amino acid identity\",\n",
                "                 fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"UMAP-1\")\n",
                "    ax.set_ylabel(\"UMAP-2\")\n",
                "\n",
                "    # Right: colour by source protein\n",
                "    ax = axes[1]\n",
                "    source_labels = np.array(all_source)\n",
                "    unique_sources = list(sequences.keys())\n",
                "    scmap = plt.cm.get_cmap(\"Set2\", len(unique_sources))\n",
                "    for i, src in enumerate(unique_sources):\n",
                "        mask = source_labels == src\n",
                "        ax.scatter(Z[mask, 0], Z[mask, 1], c=[scmap(i)],\n",
                "                   label=src, s=40, alpha=0.75, edgecolors=\"white\", linewidths=0.3)\n",
                "    ax.legend(title=\"Protein\", bbox_to_anchor=(1.0, 1), loc=\"upper left\",\n",
                "              fontsize=8, title_fontsize=9)\n",
                "    ax.set_title(\"UMAP of ESM-2 residue embeddings\\nColoured by source protein\",\n",
                "                 fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"UMAP-1\")\n",
                "    ax.set_ylabel(\"UMAP-2\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    print(\"\"\"\n",
                "What to look for:\n",
                "  â€¢ Same amino acid in different proteins â†’ clusters together   (ESM-2 learned residue identity)\n",
                "  â€¢ Points from the same protein â†’ may also cluster             (ESM-2 learned local context)\n",
                "  â€¢ Poly-Ala / Poly-Val / Poly-Gly â†’ tight single-residue dots (all identical context within each)\n",
                "\"\"\")\n",
                "\n",
                "except ImportError:\n",
                "    print(\"umap-learn not installed. Run: pip install umap-learn\")\n",
                "    print(\"Then re-run this cell.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Linear Secondary Structure Probe\n",
                "\n",
                "Can a **single linear layer** on top of ESM-2 embeddings predict secondary structure?\n",
                "\n",
                "We test this with **synthetic labelled data** from `synth-pdb`:\n",
                "- Generate 60 helical residues (`Poly-Ala`) â†’ label 0\n",
                "- Generate 60 strand residues (`Poly-Val`) â†’ label 1\n",
                "- Generate 60 coil residues (`Poly-Gly`) â†’ label 2\n",
                "\n",
                "Then train a logistic regression (which is just a linear layer + softmax) and evaluate accuracy.\n",
                "\n",
                "> **Educational point**: this test measures whether secondary structure information is\n",
                "> **linearly accessible** in the embedding space â€” i.e., can it be extracted with a\n",
                "> single matrix multiplication? If yes, it's explicitly encoded, not buried in non-linear\n",
                "> interactions. Literature: ESM-2 achieves ~70-80% accuracy on real protein SS3 datasets\n",
                "> with a linear probe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import warnings\n",
                "\n",
                "# Build a labelled dataset of residue embeddings\n",
                "# Label 0 = helix (Ala)  |  Label 1 = strand (Val)  |  Label 2 = coil (Gly)\n",
                "probe_seqs = [\n",
                "    (\"A\" * 30, 0, \"Helix (Ala)\"),\n",
                "    (\"V\" * 30, 1, \"Strand (Val)\"),\n",
                "    (\"G\" * 30, 2, \"Coil (Gly)\"),\n",
                "    # Additional diversity: mixed sequences\n",
                "    (\"AAAVVVAAA\" * 3, None, \"Mixed\"),   # ground truth is ambiguous, skip\n",
                "]\n",
                "\n",
                "X_probe, y_probe = [], []\n",
                "for seq, label, desc in probe_seqs:\n",
                "    if label is None:\n",
                "        continue\n",
                "    emb = embedder.embed(seq)   # (L, 320)\n",
                "    for row in emb:\n",
                "        X_probe.append(row)\n",
                "        y_probe.append(label)\n",
                "\n",
                "X_probe = np.array(X_probe)\n",
                "y_probe = np.array(y_probe)\n",
                "print(f\"Dataset: {X_probe.shape[0]} residues Ã— {X_probe.shape[1]} embedding dims\")\n",
                "print(f\"Classes: 0=Helix ({np.sum(y_probe==0)}), 1=Strand ({np.sum(y_probe==1)}), 2=Coil ({np.sum(y_probe==2)})\")\n",
                "\n",
                "# Standardise features (logistic regression converges better)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_probe)\n",
                "\n",
                "# 5-fold cross-validation with a linear probe\n",
                "with warnings.catch_warnings():\n",
                "    warnings.simplefilter(\"ignore\")\n",
                "    clf = LogisticRegression(max_iter=500, C=1.0, random_state=42)\n",
                "    cv_scores = cross_val_score(clf, X_scaled, y_probe, cv=5, scoring=\"accuracy\")\n",
                "\n",
                "print(f\"\\n5-fold CV accuracy: {cv_scores.mean():.1%} Â± {cv_scores.std():.1%}\")\n",
                "print(f\"Individual folds: {[f'{s:.1%}' for s in cv_scores]}\")\n",
                "\n",
                "# Fit on full data and show confusion matrix\n",
                "clf.fit(X_scaled, y_probe)\n",
                "y_pred = clf.predict(X_scaled)\n",
                "\n",
                "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
                "cm = confusion_matrix(y_probe, y_pred)\n",
                "fig, ax = plt.subplots(figsize=(5, 4))\n",
                "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Helix (Ala)\", \"Strand (Val)\", \"Coil (Gly)\"])\n",
                "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
                "ax.set_title(f\"Linear Probe on ESM-2 Embeddings\\nCV accuracy: {cv_scores.mean():.1%}\",\n",
                "             fontweight=\"bold\", pad=10)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\"\"\n",
                "Interpretation:\n",
                "  High accuracy â†’ secondary structure is LINEARLY ENCODED in ESM-2 embeddings.\n",
                "  The model learned to distinguish helix/strand/coil from sequence context alone,\n",
                "  with no 3D coordinates in the training data.\n",
                "  This is the same principle that enables ESMFold to predict full 3D structure.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Embedding a Structure from synth-pdb\n",
                "\n",
                "We can also pass a **biotite `AtomArray`** directly â€” `embed_structure()` extracts\n",
                "the amino acid sequence and calls `embed()` under the hood.\n",
                "\n",
                "This is the natural integration point with `synth-pdb`'s structure generator."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from synth_pdb.generator import generate_pdb_content\n",
                "import biotite.structure.io.pdb as pdb\n",
                "import biotite.structure as struc\n",
                "import io\n",
                "\n",
                "# Generate a synthetic 20-residue alpha-helix\n",
                "pdb_content = generate_pdb_content(\n",
                "    sequence_str=\"ALA-ALA-ALA-ALA-ALA-GLY-LEU-ALA-ALA-ALA-ALA-ALA-SER-ALA-ALA-ALA-ALA-ALA-ALA-ALA\",\n",
                "    ss_type=\"helix\",\n",
                "    minimize_energy=False,\n",
                ")\n",
                "\n",
                "# Parse into a biotite AtomArray\n",
                "pdb_file = pdb.PDBFile.read(io.StringIO(pdb_content))\n",
                "structure = pdb_file.get_structure(model=1)\n",
                "# Get backbone only\n",
                "backbone = structure[np.isin(structure.atom_name, [\"N\", \"CA\", \"C\", \"O\"])]\n",
                "\n",
                "print(f\"Generated structure: {len(struc.get_residues(backbone)[0])} residues, {len(backbone)} atoms\")\n",
                "\n",
                "# Embed directly from the AtomArray\n",
                "struct_emb = embedder.embed_structure(backbone)   # (L, 320)\n",
                "print(f\"PLM embedding shape: {struct_emb.shape}\")\n",
                "print(f\"dtype: {struct_emb.dtype}\")\n",
                "\n",
                "# Plot the embedding\n",
                "fig, ax = plt.subplots(figsize=(12, 3))\n",
                "im = ax.imshow(struct_emb.T, aspect=\"auto\", cmap=\"RdBu_r\",\n",
                "               interpolation=\"nearest\")\n",
                "ax.set_xlabel(\"Residue position\", fontsize=10)\n",
                "ax.set_ylabel(\"ESM-2 dimension (320)\", fontsize=10)\n",
                "ax.set_title(\"synth-pdb â†’ ESM-2 embedding:  20-residue synthetic helix\",\n",
                "             fontsize=11, fontweight=\"bold\")\n",
                "plt.colorbar(im, ax=ax, label=\"Activation\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nThe embedder extracted the sequence from the AtomArray residue names,\")\n",
                "print(\"passed it through ESM-2, and returned a (L, 320) float32 matrix.\")\n",
                "print(\"This can be used directly as node features in a GNN.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary & Next Steps\n",
                "\n",
                "### What we demonstrated\n",
                "\n",
                "| Step | Result |\n",
                "|---|---|\n",
                "| `embedder.embed(seq)` | `(L, 320)` per-residue float32 matrix |\n",
                "| `embedder.embed_structure(arr)` | Same, directly from `AtomArray` |\n",
                "| Heatmaps | Poly-Ala uniform; ubiquitin diverse â€” context is encoded |\n",
                "| Similarity matrix | Repetitive peptides cluster; real proteins moderate similarity |\n",
                "| UMAP | Residues cluster by amino acid identity in 2D |\n",
                "| Linear probe | High SS accuracy â†’ structural info **linearly accessible** |\n",
                "\n",
                "### Using a more powerful model\n",
                "\n",
                "The API is identical for all ESM-2 variants:\n",
                "\n",
                "```python\n",
                "# Current: 8M params, 320-dim\n",
                "embedder = ESM2Embedder()   \n",
                "\n",
                "# Better: 35M params, 480-dim (~140 MB)\n",
                "embedder = ESM2Embedder(model_name=\"facebook/esm2_t12_35M_UR50D\")\n",
                "\n",
                "# Best for research: 650M params, 1280-dim (~2.5 GB, use GPU)\n",
                "embedder = ESM2Embedder(model_name=\"facebook/esm2_t33_650M_UR50D\", device=\"cuda\")\n",
                "```\n",
                "\n",
                "### Downstream integration ideas\n",
                "\n",
                "```python\n",
                "# 1. Feed into the synth-pdb GNN quality scorer as node features\n",
                "plm_feats = embedder.embed_structure(structure)      # (L, 320)\n",
                "node_features = np.concatenate([geo_feats, plm_feats], axis=-1)\n",
                "\n",
                "# 2. Pairwise contact prediction (outer product trick)\n",
                "emb = embedder.embed(sequence)                       # (L, 320)\n",
                "outer = np.einsum('id,jd->ij', emb, emb)            # (L, L) â€” feed into CNN\n",
                "\n",
                "# 3. Retrieval by function similarity\n",
                "sim = embedder.sequence_similarity(query_seq, candidate_seq)\n",
                "```\n",
                "\n",
                "### Reference\n",
                "\n",
                "Lin, Z. et al. (2023) *Evolutionary-scale prediction of atomic-level protein structure with a language model.* Science 379, 1123â€“1130.  \n",
                "https://doi.org/10.1126/science.ade2574"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true,
            "name": "plm_embeddings.ipynb"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
